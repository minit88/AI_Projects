{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/minit88/ML-Data_Analysis/blob/main/Price_Detection(cabbage)/Price_Detection(cabbage)(myself).ipynb",
      "authorship_tag": "ABX9TyMhPWUzzF51gomI5MhG3e+H"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "eiEO6DmwtW1s"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import csv\n",
        "from pandas.io.parsers import read_csv\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "from torch.utils.data import TensorDataset # 텐서데이터셋\n",
        "from torch.utils.data import DataLoader # 데이터로더\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#dataes =  open('/content/test_data.csv','r')\n",
        "#dataes = csv.reader(dataes)\n",
        "#train_data = []\n",
        "\n",
        "#for data in dataes:\n",
        "  #train_data.append([data[5],data[8],data[10]])\n",
        "#print(train_data[1:])\n",
        "\n",
        "\n",
        "\n",
        "data = read_csv('/content/drive/MyDrive/test03.csv',sep=',',thousands=',' )\n",
        "\n",
        "xy=np.array(data,dtype=np.float32)\n",
        "\n",
        "# print(data[5][1:][1].replace(',',''))\n",
        "\n",
        "#print(float(data[1][2]))\n",
        "#print(data[1:][5],data[1:][8],data[1:][10])\n",
        "#data = [data[1:][5],data[1:][8],data[1:][10]]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[0:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDlA9d7z1Dg0",
        "outputId": "6a4c110e-4efb-484a-9d33-2fd88c867d49"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     전용면적(㎡)    계약년월  계약일   층  건축년도  거래금액(만원)\n",
            "0    70.9700  202208   25   9  2017     65500\n",
            "1    55.9200  202208    8  15  2015     26900\n",
            "2   100.4300  202208   19  17  2020    140000\n",
            "3    73.6700  202208   29   8  2020     77900\n",
            "4    50.7624  202208   25  13  2013     28000\n",
            "5    28.9500  202208    6   7  2016     13600\n",
            "6    60.5900  202208    3   5  2002     23000\n",
            "7    76.5000  202208    4   5  2003     24800\n",
            "8    72.0050  202208    8   6  2004     26000\n",
            "9    59.7600  202208    4   7  1996     29990\n",
            "10  112.8180  202208   27  13  2005     78000\n",
            "11   27.3600  202208    4  13  2004      7850\n",
            "12   23.0400  202208   22  20  2004      7900\n",
            "13   84.9870  202208   25   4  2005     67000\n",
            "14   51.5883  202208    6   8  2017     25000\n",
            "15   43.3300  202208    2   7  2013     13500\n",
            "16   21.6400  202208    2   6  2013      6500\n",
            "17   84.4885  202208   22  10  2020     58960\n",
            "18   79.0200  202208   26   3  1987     99500\n",
            "19   73.9200  202208    8   1  1979    107000\n",
            "20   61.0900  202208   13   6  1979     95000\n",
            "21  179.4912  202208   23  26  2007    115000\n",
            "22  139.1100  202208   10   4  1993     32500\n",
            "23   84.4300  202208   16  14  1993     23000\n",
            "24   62.6471  202208   15  14  2017     40500\n",
            "25  133.9400  202208   11   9  1991     57000\n",
            "26   59.9694  202208   13   4  2019     46000\n",
            "27   48.9900  202208   15   3  1977     16000\n",
            "28   84.9357  202208   27   7  2004     54000\n",
            "29   84.7320  202208    4  24  2005     53000\n",
            "30   84.9900  202208   22   1  1993     23000\n",
            "31   84.7270  202208   11  25  2016     74000\n",
            "32   68.7083  202208   27   7  2014     49000\n",
            "33   41.1352  202208   14   7  2016     15900\n",
            "34   67.6200  202208   20   2  1984     21900\n",
            "35   59.9415  202208    5  17  2014     65000\n",
            "36  109.1877  202208    8  19  2017    120000\n",
            "37  150.8000  202208   30  31  2003    103000\n",
            "38   84.9710  202208   22  16  2006     79000\n",
            "39   59.3600  202208    5   6  1997     41000\n",
            "40  101.0800  202208    3   5  2000     41000\n",
            "41   84.9394  202208    2  15  2014     60000\n",
            "42   84.2100  202208    5   5  1988    120000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oR75PYYMvjEP",
        "outputId": "46302d67-6558-4e11-bf8e-3a3c2f4c76f0"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[7.097000e+01 2.022080e+05 2.500000e+01 9.000000e+00 2.017000e+03\n",
            "  6.550000e+04]\n",
            " [5.592000e+01 2.022080e+05 8.000000e+00 1.500000e+01 2.015000e+03\n",
            "  2.690000e+04]\n",
            " [1.004300e+02 2.022080e+05 1.900000e+01 1.700000e+01 2.020000e+03\n",
            "  1.400000e+05]\n",
            " [7.367000e+01 2.022080e+05 2.900000e+01 8.000000e+00 2.020000e+03\n",
            "  7.790000e+04]\n",
            " [5.076240e+01 2.022080e+05 2.500000e+01 1.300000e+01 2.013000e+03\n",
            "  2.800000e+04]\n",
            " [2.895000e+01 2.022080e+05 6.000000e+00 7.000000e+00 2.016000e+03\n",
            "  1.360000e+04]\n",
            " [6.059000e+01 2.022080e+05 3.000000e+00 5.000000e+00 2.002000e+03\n",
            "  2.300000e+04]\n",
            " [7.650000e+01 2.022080e+05 4.000000e+00 5.000000e+00 2.003000e+03\n",
            "  2.480000e+04]\n",
            " [7.200500e+01 2.022080e+05 8.000000e+00 6.000000e+00 2.004000e+03\n",
            "  2.600000e+04]\n",
            " [5.976000e+01 2.022080e+05 4.000000e+00 7.000000e+00 1.996000e+03\n",
            "  2.999000e+04]\n",
            " [1.128180e+02 2.022080e+05 2.700000e+01 1.300000e+01 2.005000e+03\n",
            "  7.800000e+04]\n",
            " [2.736000e+01 2.022080e+05 4.000000e+00 1.300000e+01 2.004000e+03\n",
            "  7.850000e+03]\n",
            " [2.304000e+01 2.022080e+05 2.200000e+01 2.000000e+01 2.004000e+03\n",
            "  7.900000e+03]\n",
            " [8.498700e+01 2.022080e+05 2.500000e+01 4.000000e+00 2.005000e+03\n",
            "  6.700000e+04]\n",
            " [5.158830e+01 2.022080e+05 6.000000e+00 8.000000e+00 2.017000e+03\n",
            "  2.500000e+04]\n",
            " [4.333000e+01 2.022080e+05 2.000000e+00 7.000000e+00 2.013000e+03\n",
            "  1.350000e+04]\n",
            " [2.164000e+01 2.022080e+05 2.000000e+00 6.000000e+00 2.013000e+03\n",
            "  6.500000e+03]\n",
            " [8.448850e+01 2.022080e+05 2.200000e+01 1.000000e+01 2.020000e+03\n",
            "  5.896000e+04]\n",
            " [7.902000e+01 2.022080e+05 2.600000e+01 3.000000e+00 1.987000e+03\n",
            "  9.950000e+04]\n",
            " [7.392000e+01 2.022080e+05 8.000000e+00 1.000000e+00 1.979000e+03\n",
            "  1.070000e+05]\n",
            " [6.109000e+01 2.022080e+05 1.300000e+01 6.000000e+00 1.979000e+03\n",
            "  9.500000e+04]\n",
            " [1.794912e+02 2.022080e+05 2.300000e+01 2.600000e+01 2.007000e+03\n",
            "  1.150000e+05]\n",
            " [1.391100e+02 2.022080e+05 1.000000e+01 4.000000e+00 1.993000e+03\n",
            "  3.250000e+04]\n",
            " [8.443000e+01 2.022080e+05 1.600000e+01 1.400000e+01 1.993000e+03\n",
            "  2.300000e+04]\n",
            " [6.264710e+01 2.022080e+05 1.500000e+01 1.400000e+01 2.017000e+03\n",
            "  4.050000e+04]\n",
            " [1.339400e+02 2.022080e+05 1.100000e+01 9.000000e+00 1.991000e+03\n",
            "  5.700000e+04]\n",
            " [5.996940e+01 2.022080e+05 1.300000e+01 4.000000e+00 2.019000e+03\n",
            "  4.600000e+04]\n",
            " [4.899000e+01 2.022080e+05 1.500000e+01 3.000000e+00 1.977000e+03\n",
            "  1.600000e+04]\n",
            " [8.493570e+01 2.022080e+05 2.700000e+01 7.000000e+00 2.004000e+03\n",
            "  5.400000e+04]\n",
            " [8.473200e+01 2.022080e+05 4.000000e+00 2.400000e+01 2.005000e+03\n",
            "  5.300000e+04]\n",
            " [8.499000e+01 2.022080e+05 2.200000e+01 1.000000e+00 1.993000e+03\n",
            "  2.300000e+04]\n",
            " [8.472700e+01 2.022080e+05 1.100000e+01 2.500000e+01 2.016000e+03\n",
            "  7.400000e+04]\n",
            " [6.870830e+01 2.022080e+05 2.700000e+01 7.000000e+00 2.014000e+03\n",
            "  4.900000e+04]\n",
            " [4.113520e+01 2.022080e+05 1.400000e+01 7.000000e+00 2.016000e+03\n",
            "  1.590000e+04]\n",
            " [6.762000e+01 2.022080e+05 2.000000e+01 2.000000e+00 1.984000e+03\n",
            "  2.190000e+04]\n",
            " [5.994150e+01 2.022080e+05 5.000000e+00 1.700000e+01 2.014000e+03\n",
            "  6.500000e+04]\n",
            " [1.091877e+02 2.022080e+05 8.000000e+00 1.900000e+01 2.017000e+03\n",
            "  1.200000e+05]\n",
            " [1.508000e+02 2.022080e+05 3.000000e+01 3.100000e+01 2.003000e+03\n",
            "  1.030000e+05]\n",
            " [8.497100e+01 2.022080e+05 2.200000e+01 1.600000e+01 2.006000e+03\n",
            "  7.900000e+04]\n",
            " [5.936000e+01 2.022080e+05 5.000000e+00 6.000000e+00 1.997000e+03\n",
            "  4.100000e+04]\n",
            " [1.010800e+02 2.022080e+05 3.000000e+00 5.000000e+00 2.000000e+03\n",
            "  4.100000e+04]\n",
            " [8.493940e+01 2.022080e+05 2.000000e+00 1.500000e+01 2.014000e+03\n",
            "  6.000000e+04]\n",
            " [8.421000e+01 2.022080e+05 5.000000e+00 5.000000e+00 1.988000e+03\n",
            "  1.200000e+05]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Pfez-1qOlI7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "random.seed(777)\n",
        "torch.manual_seed(777)\n",
        "if device =='cuda':\n",
        "  torch.cuda.manual_seed_all(777)"
      ],
      "metadata": {
        "id": "j5AdS6Di0Sfv"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "training_epochs = 100\n"
      ],
      "metadata": {
        "id": "tM9BKw8i2nwZ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4개의 변인을 입력을 받습니다.\n",
        "# avgTemp,minTemp,maxTemp,rainFall\n",
        "x_data =torch.FloatTensor(xy[:,0:-1])\n",
        "\n",
        "\n",
        "# 가격 값을 입력 받습니다.\n",
        "# avgPrice\n",
        "y_data =torch.FloatTensor(xy[:,[-1]])\n",
        "dataset = TensorDataset(x_data,y_data)\n",
        "data_loader = torch.utils.data.DataLoader(dataset,batch_size = 1,shuffle=True,drop_last=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "AmKpDtjlyMBo"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nn layers\n",
        "linear1 = torch.nn.Linear(5,1,bias=True)\n",
        "\n",
        "\n",
        "relu = torch.nn.ReLU()\n",
        "\n",
        "# xavier initialization#\n",
        "torch.nn.init.xavier_uniform_(linear1.weight)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwKR3Glk0KGJ",
        "outputId": "bf54335f-d129-4a2c-b676-c756e5434fce"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.4485,  0.0065,  0.6497, -0.4731, -0.3775]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model \n",
        "model = torch.nn.Sequential(linear1,relu).to(device)"
      ],
      "metadata": {
        "id": "oTD3SgSO17nX"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define cost/loss & optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = 0.01)"
      ],
      "metadata": {
        "id": "sUFCdmYe2Fcx"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_batch = len(data_loader)\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "\n",
        "    for X, Y in data_loader:\n",
        "        #print(X,Y)\n",
        "        # reshape input image into [batch_size by 784]\n",
        "        # label is not one-hot encoded\n",
        "        n=len(xy)\n",
        "        hypothesis = model(X)\n",
        "        #print(hypothesis)\n",
        "        cost = F.mse_loss(hypothesis,Y)\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning finished')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0p530Ad2dAi",
        "outputId": "bf66e434-e792-4d2c-acdf-5b68d986139c"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 cost = 1924770688.000000000\n",
            "Epoch: 0002 cost = 1287355008.000000000\n",
            "Epoch: 0003 cost = 1293319552.000000000\n",
            "Epoch: 0004 cost = 1286227200.000000000\n",
            "Epoch: 0005 cost = 1282249472.000000000\n",
            "Epoch: 0006 cost = 1258417536.000000000\n",
            "Epoch: 0007 cost = 1278318592.000000000\n",
            "Epoch: 0008 cost = 1293795840.000000000\n",
            "Epoch: 0009 cost = 1287275008.000000000\n",
            "Epoch: 0010 cost = 1291671552.000000000\n",
            "Epoch: 0011 cost = 1257786240.000000000\n",
            "Epoch: 0012 cost = 1279563776.000000000\n",
            "Epoch: 0013 cost = 1271743616.000000000\n",
            "Epoch: 0014 cost = 1297723264.000000000\n",
            "Epoch: 0015 cost = 1314477696.000000000\n",
            "Epoch: 0016 cost = 1295598464.000000000\n",
            "Epoch: 0017 cost = 1362978048.000000000\n",
            "Epoch: 0018 cost = 1303181056.000000000\n",
            "Epoch: 0019 cost = 1299439104.000000000\n",
            "Epoch: 0020 cost = 1319419008.000000000\n",
            "Epoch: 0021 cost = 1266754688.000000000\n",
            "Epoch: 0022 cost = 1276658048.000000000\n",
            "Epoch: 0023 cost = 1278288000.000000000\n",
            "Epoch: 0024 cost = 1278516352.000000000\n",
            "Epoch: 0025 cost = 1281754496.000000000\n",
            "Epoch: 0026 cost = 1280154624.000000000\n",
            "Epoch: 0027 cost = 1298020736.000000000\n",
            "Epoch: 0028 cost = 1249299328.000000000\n",
            "Epoch: 0029 cost = 1279495040.000000000\n",
            "Epoch: 0030 cost = 1274197248.000000000\n",
            "Epoch: 0031 cost = 1302153216.000000000\n",
            "Epoch: 0032 cost = 1259881856.000000000\n",
            "Epoch: 0033 cost = 1290953856.000000000\n",
            "Epoch: 0034 cost = 1273920896.000000000\n",
            "Epoch: 0035 cost = 1263345152.000000000\n",
            "Epoch: 0036 cost = 1281590400.000000000\n",
            "Epoch: 0037 cost = 1275091328.000000000\n",
            "Epoch: 0038 cost = 1284967040.000000000\n",
            "Epoch: 0039 cost = 1284398464.000000000\n",
            "Epoch: 0040 cost = 1259785728.000000000\n",
            "Epoch: 0041 cost = 1256339840.000000000\n",
            "Epoch: 0042 cost = 1264298496.000000000\n",
            "Epoch: 0043 cost = 1272063488.000000000\n",
            "Epoch: 0044 cost = 1266366080.000000000\n",
            "Epoch: 0045 cost = 1274102528.000000000\n",
            "Epoch: 0046 cost = 1289900416.000000000\n",
            "Epoch: 0047 cost = 1253104256.000000000\n",
            "Epoch: 0048 cost = 1284676736.000000000\n",
            "Epoch: 0049 cost = 1277346560.000000000\n",
            "Epoch: 0050 cost = 1264903040.000000000\n",
            "Epoch: 0051 cost = 1270562432.000000000\n",
            "Epoch: 0052 cost = 1307984768.000000000\n",
            "Epoch: 0053 cost = 1299216000.000000000\n",
            "Epoch: 0054 cost = 1276837248.000000000\n",
            "Epoch: 0055 cost = 1258569472.000000000\n",
            "Epoch: 0056 cost = 1343197440.000000000\n",
            "Epoch: 0057 cost = 1308839552.000000000\n",
            "Epoch: 0058 cost = 1274018048.000000000\n",
            "Epoch: 0059 cost = 1270663296.000000000\n",
            "Epoch: 0060 cost = 1301674112.000000000\n",
            "Epoch: 0061 cost = 1321388416.000000000\n",
            "Epoch: 0062 cost = 1358217472.000000000\n",
            "Epoch: 0063 cost = 1298705408.000000000\n",
            "Epoch: 0064 cost = 1333445632.000000000\n",
            "Epoch: 0065 cost = 1291165568.000000000\n",
            "Epoch: 0066 cost = 1279075840.000000000\n",
            "Epoch: 0067 cost = 1301131904.000000000\n",
            "Epoch: 0068 cost = 1262584704.000000000\n",
            "Epoch: 0069 cost = 1323812224.000000000\n",
            "Epoch: 0070 cost = 1263794304.000000000\n",
            "Epoch: 0071 cost = 1392199936.000000000\n",
            "Epoch: 0072 cost = 1270666624.000000000\n",
            "Epoch: 0073 cost = 1299627136.000000000\n",
            "Epoch: 0074 cost = 1285787648.000000000\n",
            "Epoch: 0075 cost = 1257628672.000000000\n",
            "Epoch: 0076 cost = 1278705280.000000000\n",
            "Epoch: 0077 cost = 1289386624.000000000\n",
            "Epoch: 0078 cost = 1256381440.000000000\n",
            "Epoch: 0079 cost = 1317261824.000000000\n",
            "Epoch: 0080 cost = 1268655616.000000000\n",
            "Epoch: 0081 cost = 1285477888.000000000\n",
            "Epoch: 0082 cost = 1318160000.000000000\n",
            "Epoch: 0083 cost = 1318574464.000000000\n",
            "Epoch: 0084 cost = 1271039232.000000000\n",
            "Epoch: 0085 cost = 1257437440.000000000\n",
            "Epoch: 0086 cost = 1305290496.000000000\n",
            "Epoch: 0087 cost = 1265641216.000000000\n",
            "Epoch: 0088 cost = 1281724032.000000000\n",
            "Epoch: 0089 cost = 1309470336.000000000\n",
            "Epoch: 0090 cost = 1264357376.000000000\n",
            "Epoch: 0091 cost = 1281265664.000000000\n",
            "Epoch: 0092 cost = 1246835200.000000000\n",
            "Epoch: 0093 cost = 1308572288.000000000\n",
            "Epoch: 0094 cost = 1254415488.000000000\n",
            "Epoch: 0095 cost = 1239091584.000000000\n",
            "Epoch: 0096 cost = 1303079936.000000000\n",
            "Epoch: 0097 cost = 1227561600.000000000\n",
            "Epoch: 0098 cost = 1264490752.000000000\n",
            "Epoch: 0099 cost = 1272500736.000000000\n",
            "Epoch: 0100 cost = 1268118272.000000000\n",
            "Learning finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = read_csv('/content/test02.csv',sep=',',thousands=',' )\n",
        "\n",
        "test_xy = np.array(test_data,dtype=np.float32)\n",
        "x_data_test = torch.FloatTensor(test_xy[:,0:-1])\n"
      ],
      "metadata": {
        "id": "Jkn9RLtd3sEO"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습한 W를 train data에 적용해봄\n",
        "\n",
        "y_pred = model(x_data_test)\n",
        "for i in range(43):\n",
        "  print('예측된 가격:',int(y_pred[i].item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxR-ooYk3KwL",
        "outputId": "b919620a-6e04-4e95-efaa-d5b84c43fa0c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예측된 가격: 53044\n",
            "예측된 가격: 52802\n",
            "예측된 가격: 53330\n",
            "예측된 가격: 53095\n",
            "예측된 가격: 52876\n",
            "예측된 가격: 52474\n",
            "예측된 가격: 52743\n",
            "예측된 가격: 52904\n",
            "예측된 가격: 52899\n",
            "예측된 가격: 52757\n",
            "예측된 가격: 53489\n",
            "예측된 가격: 52484\n",
            "예측된 가격: 52633\n",
            "예측된 가격: 53147\n",
            "예측된 가격: 52698\n",
            "예측된 가격: 52581\n",
            "예측된 가격: 52366\n",
            "예측된 가격: 53155\n",
            "예측된 가격: 53094\n",
            "예측된 가격: 52888\n",
            "예측된 가격: 52838\n",
            "예측된 가격: 54182\n",
            "예측된 가격: 53548\n",
            "예측된 가격: 53136\n",
            "예측된 가격: 52916\n",
            "예측된 가격: 53539\n",
            "예측된 가격: 52808\n",
            "예측된 가격: 52718\n",
            "예측된 가격: 53182\n",
            "예측된 가격: 53107\n",
            "예측된 가격: 53105\n",
            "예측된 가격: 53168\n",
            "예측된 가격: 53025\n",
            "예측된 가격: 52656\n",
            "예측된 가격: 52930\n",
            "예측된 가격: 52830\n",
            "예측된 가격: 53339\n",
            "예측된 가격: 53996\n",
            "예측된 가격: 53201\n",
            "예측된 가격: 52754\n",
            "예측된 가격: 53132\n",
            "예측된 가격: 53033\n",
            "예측된 가격: 52988\n"
          ]
        }
      ]
    }
  ]
}